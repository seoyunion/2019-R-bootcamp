head(bias.unemp)
donations <- data.table(
amt = c(99,100,5,15,11,1200),
dt = as.Date(c("2019-2-27", "2019-3-2", "2019-6-13", "2019-8-1", "2019-8-31", "2019-9-15"))
)
publicity <- data.table(
identifier = c("q4q42", "4299hj", "bbg2"), #캠페인 아이디
dt = as.Date(c("2019-1-1", "2019-4-1", "2019-7-1")) #캠페인이 시작된 날짜
)
#확인
donations
publicity
#rolling join 2번째 방법
setkey(publicity, 'dt')
setkey(donations, 'dt')
publicity[donations, roll=TRUE] #이벤트를 기부에 대해 롤링: 각각의 캠페인 별로 기부금이 얼마나 들어왔는 지 확인 가능
#누락된 값 채우는 가장 간단한 방법
#바로 앞의 값으로 채워줌
rand.unemp[, impute.ff := na.locf(UNRATE, na.rm=FALSE)] #impute.ff 열 추가, UNRATE변수를 na.locf함수로 forwardfill해줌
bias.unemp[, impute.ff := na.locf(UNRATE, na.rm=FALSE)]
#확인
head(rand.unemp) #열 추가됨
head(bias.unemp)
#plotting
unemp[350:400, plot(DATE, UNRATE, col=1, lwd=2, type='b')] #range=350~400, col=색깔, lwd=line width
rand.unemp[350:400, lines(DATE, impute.ff, col=1, lwd=2, lty=2)] #
rand.unemp[350:400][rpt==TRUE, points(DATE, impute.ff, col=2, pch=6, cex=2)] #rpt==TRUE: NA값이었던 것들 표시
rand.unemp <- rand.unemp[, impute.rm.nolkh := rollapply(c(NA,NA,UNRATE), 3, #없는 값 앞의 두개를 보겠다
function(x) {
if (!is.na(x[3])) x[3] else mean(x, na.rm = TRUE) #NA값 무시하고 세 개의 값의 평균 구하기 위함
})]
bias.unemp <- bias.unemp[, impute.rm.nolkh := rollapply(c(NA,NA,UNRATE), 3,
function(x) {
if (!is.na(x[3])) x[3] else mean(x, na.rm = TRUE)
})]
#확인
head(rand.unemp)
head(bias.unemp)
##누락된 데이터 이전, 이후 데이터의 값을 통해 계산하는 것이 더 좋은 경우도 있다.(예측 모델에는 적함X)
##-------- Lookahead MA --------
rand.unemp[, complete.rm := rollapply(c(NA, UNRATE, NA), 3, #룩어헤드가 있는 이동평균
function(x) {
if (!is.na(x[2]))
x[2]
else {
mean(x, na.rm=TRUE)
}
})]
head(rand.unemp)
rand.unemp[350:400][rpt==TRUE, points(DATE, complete.rm, col=2, pch=6, cex=2)]
## -------linear interpolation --------
rand.unemp[, impute.li := na.approx(UNRATE)]
bias.unemp[, impute.li := na.approx(UNRATE)]
## ------ Ploynomial interpolation --------
rand.unemp[, impute.sp := na.spline(UNRATE)]
bias.unemp[, impute.sp := na.spline(UNRATE)]
head(rand.unemp)
#plotting
use.idx = 90:120 #인덱스로 사용할 범위
unemp[use.idx, plot(DATE, UNRATE, col=1, type='b')]
rand.unemp[use.idx, lines(DATE, impute.li, col=2, lwd=2, lty=2)]
rand.unemp[use.idx, lines(DATE, impute.sp, col=3, lwd=2, lty=3)]
sort(rand.unemp[, lapply(.SD, #표준편차 SD를 기준으로 정렬
function(x)
mean((x -unemp$UNRATE)^2, na.rm=TRUE)),
.SDcols = c("impute.ff", "impute.rm.nolkh", "impute.li", "impute.sp", "complete.rm")
]) #표준편차를 구할 컬럼들
sort(rand.unemp[, lapply(.SD, #표준편차 SD를 기준으로 정렬
function(x)
mean((x -unemp$UNRATE)^2, na.rm=TRUE)),
.SDcols = c("impute.ff", "impute.rm.nolkh", "impute.li", "impute.sp", "complete.rm")
]) #표준편차를 구할 컬럼들
require(data.table)
require(zoo)
AirPassengers <- fread("AirPassengers.csv")
head(AirPassengers)
#Plotting하기 위해 Time Series data로 변환(ts)
AirPassengers.ts = ts(
data=AirPassengers$V2,
start=as.yearmon(AirPassengers[1]$V1),  #날짜 형식으로 변환해주는 zoo 안의 함수 "as.yearmon"
end=as.yearmon(tail(AirPassengers)[6]$V1),
frequency=12
)
# stl(): 계절성과 트렌드를 분리해주는 함수(Loess 사용)
plot(stl(AirPassengers.ts, 'periodic'))
####ck week 3####
###Missing Data Construction###
getwd()
setwd("C:/Users/김서윤/Documents/2020 2학기 강의/응용시계열분석(임미영)/R 실습")
#Imputation : 결측값 대체
#Interpolation : 인접 데이터포인트 사용하여 누락된 값 추정
#Deletion of affected time periods: 비추천
#install.packages("devtools")
library(devtools)
require(data.table)
require(zoo)
unemp <- fread("UNRATE.csv")
unemp[, DATE := as.Date(DATE)]
head(unemp) #DATE 열을 날짜 형식으로 변환
## generate a data set where data is randomly missing
rand.unemp.idx <- sample(1:nrow(unemp), .1*nrow(unemp)) #1부터 전체 행 개수 중 10% 만큼의 개수를 샘플링
rand.unemp <- unemp[-rand.unemp.idx] #랜덤한 unemp데이터에서 방금 샘플링 한 데이터를 빼줌
nrow(unemp)
nrow(rand.unemp)
## generate a data set
## where data is more likely to be missing if it's high
high.unemp.idx <- which(unemp$UNRATE > 8) #8보다 높은 실업률 갖는 데이터 중 반 샘플링
high.unemp.idx <- sample(high.unemp.idx, .5*length(high.unemp.idx))
bias.unemp <- unemp[-high.unemp.idx]
nrow(bias.unemp)
#######[data.table Package]#######
##dt[, new.col := old.col + 7]
##dt[, .(col1, col2, col3)]   #######열을 둘러싸는 ".()" : 리스트를 빨리 작성하는 법
##dt[col1 < 3 & col2 >5]
##dt[col1 < 3 & col2>5,. (col1, col2, col3)]
##dt[, .N, col1]  ##### ".N" : 행 계수 계산
all.dates <- seq(from = unemp$DATE[1], to=tail(unemp$DATE, 1), #DATE변수의 첫번째부터 마지막까지
by="month") #월별로 추출
#확인
head(unemp)
tail(unemp)
unemp$DATE[1]
rand.unemp = rand.unemp[J(all.dates), on='DATE', roll=0] #DATE라는 변수에 대해서
bias.unemp = bias.unemp[J(all.dates), on='DATE', roll=0] ###????### roll??
rand.unemp[, rpt := is.na(UNRATE)] #rpt라는 이름으로 rand.unemp변수 바로 뒤에 is.na(T or F)값을 부착하라
#확인
nrow(rand.unemp)
nrow(bias.unemp)
head(rand.unemp)
head(bias.unemp)
##-------------------DANATION DATA AS EXAMPLE---------------
###Missing Data Interpolation####
#Forward fill
#Moving average
#Interpolation
donations <- data.table(
amt = c(99,100,5,15,11,1200),
dt = as.Date(c("2019-2-27", "2019-3-2", "2019-6-13", "2019-8-1", "2019-8-31", "2019-9-15"))
)
publicity <- data.table(
identifier = c("q4q42", "4299hj", "bbg2"), #캠페인 아이디
dt = as.Date(c("2019-1-1", "2019-4-1", "2019-7-1")) #캠페인이 시작된 날짜
)
#rolling join 2번째 방법
setkey(publicity, 'dt')
setkey(donations, 'dt')
publicity[donations, roll=TRUE] #이벤트를 기부에 대해 롤링: 각각의 캠페인 별로 기부금이 얼마나 들어왔는 지 확인 가능
ts.length = 100 #길이
a = rep(0.5, ts.length) #가속도
x = rep(0, ts.length) #시작 position
v = rep(0, ts.length) #시작 속도
#모델링
for (ts in 2:ts.length){
x[ts] = v[ts-1]*2 +x[ts-1]+1/2*a[ts-1]^2 #Ft 위치 계산 공식
x[ts] = x[ts] + rnorm(1, sd=20) #노이즈 더해줌
v[ts] = v[ts-1]+2*a[ts-1] #속도 업데이트
}
par(mfrow=c(3,1))
plot(x, main='Position', type='l') #position 증가
plot(v, main='Velocity', type='l') #속도: 선형 증가
plot(a, main='Acceleration', type='l') #가속도 일정
#Filter
#필터 사용하여 실제값과 비슷하게 추정해보자
z = x+rnorm(ts.length, sd=300) #노이즈 섞어서
plot(x, ylim=range(c(x,z))) #플로팅: simulation한 y값
lines(z, col='blue') #노이즈가 있는 값 플로팅: 이 값이 우리가 앞에서 가정한 오류 가지고 있는 측정값임.
#이 노이즈가 있는 값(lien)을 필터링을 해서 최대한 실제 값과 가까운 값을 모델링 해볼 것
kalman.motion = function(z, Q, R, A, H){ #각각 모두 행렬로 정의됨
dimstate = dim(Q)[1]
xhatminus = array(rep(0, ts.length*dimstate), c(ts.length, dimstate)) #0으로 채워진 100차원의 행렬
xhat = array(rep(0, ts.length*dimstate), c(ts.length, dimstate)) #추정값: 그 전단계 값*A행렬
pminus = array(rep(0, ts.length*dimstate*dimstate), c(ts.length, dimstate, dimstate))
P = array(rep(0, ts.length*dimstate*dimstate), c(ts.length, dimstate, dimstate))
K = array(rep(0, ts.length*dimstate), c(ts.length, dimstate))
xhat[1, ] = rep(0, dimstate)
P[1, ,] = diag(dimstate)
for (k in 2:ts.length){
xhatminus[k, ] = A %*% matrix(xhat[k-1,]) #prediction 단계
pminus[k, , ] = A %*% P[k-1, ,] %*% t(A) +Q #predicrion 단계
K[k, ] = pminus[k, , ] %*% H %*% solve(t(H) %*% pminus[k, , ] %*% H+R)
xhat[k, ] = xhatminus[k, ] +K[k, ] %*% (z[k]-t(H) %*% xhatminus[k,])
P[k, ,] = (diag(dimstate)-K[k,] %*% t(H)) %*% pminus[k, , ]
}
return(list(xhat=xhat, xhatminus=xhatminus)) #예측된 Xhat을 리턴해줌
} #칼만 필터에서 정의해준 값으로 계속해서 업데이트 해나가게 됨
R = 10^2
Q = 10
A = matrix(1)
#H = matrix(4,4,4)
H  = matrix(1)
# 각각이 행렬로 정의되었고,,
xhat = kalman.motion(z, diag(1)*Q, R, A, H)[[1]] #각 값을 이렇게 넣어주고, 각각의 행렬의 dimension을 정해주어야 함
lines(xhat, col='red') #에러 뜨는데..ㅠㅠ
#### AR model simulation ####
set.seed(2016)
N =1000
phi =1 #0.4 -> 1(랜덤워크)
z=rnorm(N, 0, 1) #et 정의
x=NULL
x[1]=z[1]
#2~n까지 for문으로 xt를 채워줌
for (t in 2:N){
x[t] = z[t] + phi*x[t-1] #AR모델과 동일
} #x를 얻어냄
x.ts = ts(x) #x를 time series로 변환
par(mfrow=c(2,1)) #그릴 plot 개수 정해준다음
plot(x.ts, main=paste("AR(1) time series on white noise, phi=",toString(phi))) #plot 그려줌
x.acf = acf(x.ts, main=paste("AR(1) time series on white noise, phi=",toString(phi)))
#### AR model simulation ####
set.seed(2016)
N =1000
phi =0.4 #0.4 -> 1(랜덤워크)
z=rnorm(N, 0, 1) #et 정의
x=NULL
x[1]=z[1]
#2~n까지 for문으로 xt를 채워줌
for (t in 2:N){
x[t] = z[t] + phi*x[t-1] #AR모델과 동일
} #x를 얻어냄
x.ts = ts(x) #x를 time series로 변환
par(mfrow=c(2,1)) #그릴 plot 개수 정해준다음
plot(x.ts, main=paste("AR(1) time series on white noise, phi=",toString(phi))) #plot 그려줌
x.acf = acf(x.ts, main=paste("AR(1) time series on white noise, phi=",toString(phi)))
#### AR(2) model simulation ####
#plot 그리는 함수
plot_function <- function(x.ts, phi1, phi2) {
par(mfrow=c(2,1))
title=paste("AR(2) time series, phi1=", toString(phi1), "phi2=", toString(phi2))
plot(x.ts, main=title)
x.acf = acf(x.ts, main=title)
}
set.seed(2017)
phi1 = 0.7
phi2 = 0.2
x.ts = arima.sim(list(ar=c(phi1, phi2)), n=1000)
plot_function(x.ts, phi1, phi2)
phi1 = 0.5
phi2 = -0.4 #음수 값으로 정의
x.ts = arima.sim(list(ar=c(phi1, phi2)), n=1000)
plot_function(x.ts, phi1, phi2) #acf가 음수 값을 가지게 된다
# https://archive.ics.uci.edu/ml/datasets/Daily+Demand+Forecasting+Orders
# 여기서 데이터 정보 확인 가능
require(stats)
require(data.table)
demand = fread('C:/Users/김서윤/Documents/2020 2학기 강의/응용시계열분석(임미영)/Daily_Demand_Forecasting_Orders.csv')
demand = ts(demand[['Banking orders (2)']]) #실습에서는 데이터의 이 부분을 사용할거다
head(demand)
plot(demand)
pacf(demand)
#ar
fit <- ar(demand, method='mle') #order를 지정해주지 않으면 이 함수가 자체적으로 지정해줌
fit # fitting된 결과가 나옴. 이런 계수들을 가진다. 이 데이터로 여러 모델링을 할 수 있을 것이다?
#airima 모델을 사용할 수도 있다 -> order 3
est = arima(x=demand, order=c(3,0,0)) #order(arima,differencing degree,ma)
est #확인해보면 앞에서 봤던 것과 거의 비슷한 결과가 coefficient로 fitting되어서 나옴
est.1 = arima(x=demand, order=c(3,0,0), fixed=c(0,NA,NA,NA)) #fixed: 첫번째 codfficient를 무조건 0으로 지정
est.1 #Xt-1의 coefficient를 0으로 지정해놓고 나머지 값들을 찾아나간 결과 반환
# 훈련 데이터셋에 대해 모델성능을 검사해 이 데이터셋에 대해 어떤 모델이 적합한지 평가 가능
# 잔차의 acf를 살펴봄
# 1. est.1$residuals
acf(est.1$residuals) #유의수준을 넘어가는 잔차가 관측되면 모델 수정
# 2. LB test
Box.test(est.1$residuals, lag=10, type='Ljung', fitdf=3) #lag10안에서 correlation이 있는지 확인
# AR(p)모델을 사용한 예측
require(forecast)
plot(demand, type='l')
lines(fitted(est.1), col=3, lwd=2) #예측된 그래프
lines(fitted(est), col=2, lwd=2)
# correlation
# 예측을 했을 떄 예측한 값의 형질을 살펴보자.
# 실제 값과 예측 값의 상관관계: 높을수록 좋다.(단, 사전에 트렌드 등을 충분히 잘 제거해주어야 함)
est.vals = fitted(est) #예측된 값들
est.1.vals = fitted(est.1)
cor(diff(est.vals), diff(demand))
cor(diff(est.1.vals), diff(demand))
plot(demand, type='l')
lines(fitted(est), col=2, lwd=2) #실제 데이터의 큰 편차들을 예측하지 못하는 것을 확인
# many steps forecast
# 여러 단계를 예측해보고, 그것의 분산을 확인해보자
var(fitted(est.1,  h=3), na.rm=TRUE)
# Genenrate noise
noise = rnorm(10000) #노이즈 생성
# Introduce a var
ma_2 = NULL #ma_2 정의
# Loop for generatring MA(2) : ma_2이기 떄문에 세번쨰 값부터 채워짐
for (i in 3:10000){
ma_2[i] = noise[i] +0.7*noise[i-1]+0.2*noise[i-2] #가중치 각각 0.7, 0.2
}
# Loop for generatring MA(2) : ma_2이기 떄문에 세번쨰 값부터 채워짐
for (i in 3:10000){
ma_2[i] = noise[i] +0.7*noise[i-1]+0.2*noise[i-2] #가중치 각각 0.7, 0.2
}
# Shift data to left by 2 units
ma_process = ma_2[3:10000] #마찬가지로 앞의 두개 지우고 세번째 값부터 ma process 정의
head(ma_process) #값들이 정의되었음을 확인
# Put time series structure on a vanilla data
ma_process = ts(ma_process) #정의된 sequence를 시계열 데이터로 변환
# Partition output graphics as a multi frame of 2 rows and 1 col
par(mfrow=c(2,1))
# Plot the process and plot in ACF
plot(ma_process, main = 'A MA(2) process', ylab=' ', col='blue')
acf(ma_process, main = 'Corellogram of a MA(2) process') #lag=2에서 acf값이 cutoff됨을 확인
require(stats)
require(data.table)
require(forecast)
demand = fread('C:/Users/김서윤/Documents/2020 2학기 강의/응용시계열분석(임미영)/Daily_Demand_Forecasting_Orders.csv')
demand = ts(demand[['Banking orders (2)']])
acf(demand) #lag=3, 10일 때 유의수준에 접근하는 양상을 보임
ma.est = arima(x=demand,
order=c(0,0,10),#마지막 값이 MA process order
fixed=c(0,0,NA,rep(0,6),NA,NA) #-> 0  0 NA  0  0  0  0  0 NA NA (fixed coefficient)
) #아까 3,9번째 값이 유의수준 접근 -> NA로 세팅, 마지막 값은 intercept(절편)
#모델링된 MA3, MA9, ntercept의 coefficient 확인
print(ma.est)
#LB test
Box.test(ma.est$residuals, lag=10, type='Ljung', fitdf=3) #잔차를 이용해 Boxtest
# 예측
ma.est.val.h1 = fitted(ma.est, h=1) #1개 구간에 대해 예측
ma.est.val.h10 = fitted(ma.est, h=10) #10개 구간에 대해 예측
plot(demand)
lines(ma.est.val.h1, col=3, lwd=2)
lines(ma.est.val.h10, col=2, lwd=2) #평균에 매우 근접하게 예측 -> 이러한 통계적 모델은 긴 구간 예측에는 적합 X
# Example of Loblolly pine trees
plot(height~age, data=Loblolly)
# Simulate data
set.seed(43)
data=arima.sim(list(order=c(2,0,0), ar=c(0.7,-0.2)), n=2000) #AR(2)모델을 사용해서 데이터를 얻어냄, 데이터 수:2000
par(mfrow=c(1,2))
acf(data) #lag=3~4까지는 영향을 줌
pacf(data) #lag=2이후에는 현저하게 작아짐
# compare models
# 모델 4개 준비
ar1 = arima(data, order=c(1,0,0), include.mean=FALSE)
ar2 = arima(data, order=c(2,0,0), include.mean=FALSE)
ar3 = arima(data, order=c(3,0,0), include.mean=FALSE)
ar4 = arima(data, order=c(4,0,0), include.mean=FALSE)
#각 모델의 SSE값
sse1 = sum(resid(ar1)^2)
sse2 = sum(resid(ar2)^2)
sse3 = sum(resid(ar3)^2)
sse4 = sum(resid(ar4)^2)
print(c(sse1, sse2, sse3, sse4))
#파라미터 많을수록 패널티를 준 aic값은 어떨까
aics <- c(ar1$aic, ar2$aic, ar3$aic, ar4$aic)
plot(aics)
print(aics)
#aic값이 2일 떄 가장 낮다. 3,4 일 때 패널티를 더 줬기 때문
#aic값이 2일 떄 가장 낮다. 3,4 일 때 패널티를 더 줬기 때문
set.seed(500)
# 데이터 생성
data = arima.sim(list(order=c(1,0,1), ar=0.7, ma=0.2), n=1000000)#ARorder(p)=1, MAorder(q)=1
par(mfcol=c(3,1))
plot(data,main="ARMA(1,1) Time Series: phi1=.7, theta1=.2", xlim=c(0,400))
acf(data, main="Autocorrelation of ARMA(1,1), phi1=.7, theta1=.2") #acf: lag가 길게 이어짐. 좋지 않아...
acf(data, type="partial", main="Partial Autocorrelation of ARMA(1,1), phi1=.7, theta1=.2") #partial: pacf
# 내장 데이터 discoveries
plot(discoveries,main = "Time Series of Number of Major Scientific Discoveries in a Year")#내장데이터
stripchart(discoveries, method = "stack", offset=.5, at=.15,pch=19,
main="Number of Discoveries Dotplot",
xlab="Number of Major Scientific Discoveries in a Year",
ylab="Frequency")
par(mfcol = c(2,1))
acf(discoveries, main="ACF of Number of Major Scientific Discoveries in a Year") #lag=3까지 유의수준 넘어감
acf(discoveries, type="partial", main="PACF of Number of Major Scientific Discoveries in a Year") #lag=2에서유의수준 근접
#p와 q를 넣어서 가장 작은 aic값을 가지는 모델을 선택해보자
for (p in 0:3){
for (q in 0:3){
print(c(p, q, AIC(arima(discoveries, order=c(p, 0, q)))))
}
}
# p=1, q=1일 떄 aic 값이 440.198으로 가장 작다
arima(discoveries, order=c(1,0,1)) #모델링
plot(discoveries)
# 자동화
library(forecast)
auto.arima(discoveries, d=0) #ARIMA(1,0,1) with non-zero mean  찾아냄
#default = 'aicc'
auto.arima(discoveries, d=0, ic="bic") #bic 기반 탐색 -> 잘 찾아냄
auto.arima(discoveries, d=0, ic="aic") #aic 기반 탐색(기본값은 aicc) -> aic:잘 찾아내지 못함
# ARMA + 차분 = ARIMA 모델
require(forecast)
# ARMA모델로 시뮬레이션 데이터 만들어줌
set.seed(1017)
phi1 = 0.8
phi2 = -0.4
x.ts = arima.sim(list(ar=c(phi1, phi2), ma=c(0.2)), n=1000) #모델 정의: AR(2), MA(1) ARIMA 모델
plot(x.ts) #대체적으로 평균 일정, 트렌드도 없음.
acf(x.ts) #4까지 유의수준 넘어감, 12,24에서도
pacf(x.ts) #3, 21에서 유의수준 넘어감
#아무것도 모른다고 치고 order를 1부터 넣어서 살펴보자
ar1.ma1.md = Arima(x.ts, order=c(1,0,1)) #대체적으로 평균 일정, 트렌드 없으니 d=0
acf(x.ts) #4까지 유의수준 넘어감, 12,24에서도
pacf(x.ts) #3, 21에서 유의수준 넘어감
#아무것도 모른다고 치고 order를 1부터 넣어서 살펴보자
ar1.ma1.md = Arima(x.ts, order=c(1,0,1)) #대체적으로 평균 일정, 트렌드 없으니 d=0
par(mfrow=c(2,1))
acf(ar1.ma1.md$residuals) #모델의 잔차를 살펴봐서 자기상관을 가지고 있으면 모델의 변수를 조정해줘야 함
pacf(ar1.ma1.md$residuals)
# 둘 다 유의수준을 넘어가는 값들이 보임. -> order 조정해줘야 함
ar2.ma1.md =Arima(x.ts, order=c(2,0,1))
plot(x.ts, type='l') #실제 데이터
lines(ar2.ma1.md$fitted, col=2) #피팅된 데이터
plot(x.ts, ar2.ma1.md$fitted)
par(mfrow=c(2,1))
acf(ar2.ma1.md$residuals) #잔차의 acf
pacf(ar2.ma1.md$residuals) #잔차의 pacf
#aic값 확인
ar1.ma1.md$aic
ar2.ma1.md$aic #훨씬 작다
#얘네도 모델링을 해보자
ar2.ma2.md = Arima(x.ts, order=c(2,0,2))
plot(x.ts, type='l')
lines(ar2.ma2.md$fitted, col=2)
par(mfrow=c(2,1))
acf(ar2.ma2.md$residuals)
pacf(ar2.ma2.md$residuals)
ar1.ma1.md$aic
ar2.ma1.md$aic #얘가 그래도 쪼끔 더 낮음(학습할 파라미터가 더 적음->선호)
ar2.ma2.md$aic
#모델 비교 방법2: 실제데이터와 피팅된 데이터 상관관계 확인
cor(x.ts, ar1.ma1.md$fitted)
cor(x.ts, ar2.ma1.md$fitted)
cor(x.ts, ar2.ma2.md$fitted) #얘가 가장 비슷. 그러나 두번째 모델과 큰 차이 없으므로 두번째 모델 선택
ar2.ma1.md$coef
# 1. 교재에서 준 예시: 좋지 않지만 일단 해보자
library(forecast)
library(data.table)
demand = fread('C:/Users/김서윤/Documents/2020 2학기 강의/응용시계열분석(임미영)/Daily_Demand_Forecasting_Orders.csv')
demand = demand[['Banking orders (2)']]
est = auto.arima(demand, ic="aic",
max.p=3, max.q=9) #최대값 정해줌. 이 값 넘어가게는 order예측하지 말라는 뜻
est #est를 살펴보면 ARIMA(0,0,0) with non-zero mean 로 아무런 order가 없는 process모델. 좋지 않은 예제ㅠㅠ
plot(demand)
acf(demand) #이미 유의수준 안에 들어가있음
# 2.두 번째 데이터 사용
plot(discoveries) #내장되어있는 데이터: 트렌드 없다 -> d=0
library(forecast)
auto.arima(discoveries, d=0, approximation=FALSE) #approximation: 근사값을 취하는 과정을 시행할지말지(시행하면 더 빨라짐)
library(data.table)
library(vars)
demand = fread('C:/Users/김서윤/Documents/2020 2학기 강의/응용시계열분석(임미영)/Daily_Demand_Forecasting_Orders.csv')
head(demand)
VARselect(demand[, 11:12], lag.max=4, type='const') # Banking orders (2)와 Banking orders (3)를 선택한 데이터
est.var = VAR(demand[, 11:12], p=3, type='const') #lag=3로 해서 모델링
est.var
VARselect(demand[, 11:12], lag.max=4, type='const') # Banking orders (2)와 Banking orders (3)를 선택한 데이터
#예측값과 실제값이 얼마나 차이가 나는지 확인해보자
par(mfrow=c(2,1))
plot(demand$`Banking orders (2)`, type="l") #변수 명에 space가 있을 경우`사용
lines(fitted(est.var)[, 1], col=2) #예측된 값. 별로 좋지 않음
plot(demand$`Banking orders (3)`, type='l')
lines(fitted(est.var)[,2], col=2) # 2보다 잘 예측됨
#잔차의 acf, pacf확인: 거의다 유의수준 안에 위치 -> 더이상의 상관관계는 없어보임
par(mfrow =c(2,1))
acf(demand$`Banking orders (2)` - fitted(est.var)[, 1])
acf(demand$`Banking orders (3)` - fitted(est.var)[, 2])
# 더 정확하게 테스트 해보자
serial.test(est.var, lags.pt=8, type='PT.asymptotic') #p-value = 0.4293: 유의수준 밖이다 = 잔차 correlation이 없다. 모델이 잘 예측했다.
#데이터 못찾은 관계로 항공데이터 사용
require(bsts)
data(AirPassengers)
plot(AirPassengers)
y <- log(AirPassengers) #log transformation->분산 안정화
plot(y) #격차 줄어듬
#증가 트렌드, 연도별 계절성 보임(주기 12)
#따라서 구조모델에 트렌드, 연간 계절성 추가
ss <- AddLocalLinearTrend(list(), y)
ss <- AddSeasonal(ss, y, nseasons = 12)
training = window(y, end=c(1959, 12)) #train데이터로는 59년도까지 데이터만 사용하고 나머지 부분은 예측을 해보자
model = bsts(training, state.specification=ss, niter=100)#학습데이터 넣어주고, 파라미터 업데이트 100번 돌아라
plot(model, 'seasonal', nseasons=12) #각각의 season에 대해 대체적으로 안정된 값 보임
#모델링이 대체적으로 잘 맞아떨어졌다는 뜻
#이 모델로 예측을 해보자
pred = predict(model, horizon=12)#horizon에는 계절성 정해준 값 넣으면 됨
plot(pred)#추론된 값 그래프로 확인
pred#수치로 확인
ss
plot(ss)
source('~/2020 2학기 강의/응용시계열분석(임미영)/R 실습/bsts.R')
# Genenrate noise
noise = rnorm(10000) #노이즈 생성
# Introduce a var
ma_2 = NULL #ma_2 정의
# Loop for generatring MA(2) : ma_2이기 떄문에 세번쨰 값부터 채워짐
for (i in 3:10000){
ma_2[i] = noise[i] +0.5*noise[i-1]+0.5*noise[i-2] #가중치 각각 0.7, 0.2
}
# Shift data to left by 2 units
ma_process = ma_2[3:10000] #마찬가지로 앞의 두개 지우고 세번째 값부터 ma process 정의
head(ma_process) #값들이 정의되었음을 확인
# Put time series structure on a vanilla data
ma_process = ts(ma_process) #정의된 sequence를 시계열 데이터로 변환
# Partition output graphics as a multi frame of 2 rows and 1 col
par(mfrow=c(2,1))
# Plot the process and plot in ACF
plot(ma_process, main = 'A MA(2) process', ylab=' ', col='blue')
acf(ma_process, main = 'Corellogram of a MA(2) process') #lag=2에서 acf값이 cutoff됨을 확인
set.seed=1
model=arima.sim(n=1000, model=list(ma=c(o.5,0.5)))
model=arima.sim(n=1000, model=list(ma=c(0.5,0.5)))
model
model
acf(model)
plot(USAccDeaths)
USAccDeaths
data = USAccDeaths
data.differenced = diff(data)
plot(data.differenced)
data.differenced = diff(diff(log(data)))
plot(data.differenced)
acf(data.differenced)
pacf(data.differenced)
